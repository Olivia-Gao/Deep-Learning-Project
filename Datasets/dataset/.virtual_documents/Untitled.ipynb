import os
import pandas as pd
import numpy as np
import xml.etree.ElementTree as ET
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical



# 数据路径
train_dir = r'C:\Users\xiaog\Desktop\Project\Datasets\dataset\dataset 4\Data Files\train'
valid_dir = r'C:\Users\xiaog\Desktop\Project\Datasets\dataset\dataset 4\Data Files\valid'
test_dir = r'C:\Users\xiaog\Desktop\Project\Datasets\dataset\dataset 4\Data Files\test'

# 解析XML文件，获取标签
def get_label_from_xml(xml_file):
    tree = ET.parse(xml_file)
    root = tree.getroot()
    objects = root.findall('object')

    # 如果存在 "thermal_defect"，返回 1，否则返回 0
    for obj in objects:
        if obj.find('name').text == 'thermal_defect':
            return "1"  # 必须是字符串

    return "0"  # 也必须是字符串


def count_defects_in_directory(data_dir):
    with_defect = 0
    without_defect = 0
    
    for xml_file in os.listdir(data_dir):
        if xml_file.endswith('.xml'):
            xml_path = os.path.join(data_dir, xml_file)
            label = get_label_from_xml(xml_path)
            
            if label == "1":
                with_defect += 1
            else:
                without_defect += 1
    
    return with_defect, without_defect

# 统计数据集中有缺陷和无缺陷的图像数量
train_with_defect, train_without_defect = count_defects_in_directory(train_dir)
valid_with_defect, valid_without_defect = count_defects_in_directory(valid_dir)
test_with_defect, test_without_defect = count_defects_in_directory(test_dir)


# 处理数据，返回 DataFrame（包含文件路径和标签）
def create_dataframe(data_dir):
    data = []
    
    for xml_file in os.listdir(data_dir):
        if xml_file.endswith('.xml'):
            xml_path = os.path.join(data_dir, xml_file)
            img_path = xml_path.replace('.xml', '.jpg')  # 假设图像文件与 XML 同名
            
            if os.path.exists(img_path):  # 确保图像文件存在
                label = get_label_from_xml(xml_path)  # 确保是字符串
                data.append([img_path, label])
    
    return pd.DataFrame(data, columns=['filepath', 'label'])

# 创建数据集 DataFrame
train_df = create_dataframe(train_dir)
valid_df = create_dataframe(valid_dir)
test_df = create_dataframe(test_dir)

# 统计数据
print(f"Train set: {len(train_df)} images")
print(f"Valid set: {len(valid_df)} images")
print(f"Test set: {len(test_df)} images")

# 训练数据增强（仅用于训练集）
train_datagen = ImageDataGenerator(
    rescale=1./255,  # 归一化
    rotation_range=30,  # 随机旋转
    width_shift_range=0.2,  # 水平平移
    height_shift_range=0.2,  # 垂直平移
    shear_range=0.2,  # 剪切变换
    zoom_range=0.2,  # 缩放
    horizontal_flip=True  # 随机水平翻转
)

# 验证 & 测试数据（仅归一化，不做增强）
valid_test_datagen = ImageDataGenerator(rescale=1./255)



# 生成训练数据
train_generator = train_datagen.flow_from_dataframe(
    dataframe=train_df,
    x_col="filepath",
    y_col="label",
    target_size=(128,128),
    batch_size =8,
    class_mode="categorical"  
)

# 生成验证数据
valid_generator = valid_test_datagen.flow_from_dataframe(
    dataframe=valid_df,
    x_col="filepath",
    y_col="label",
    target_size=(128,128),
     batch_size = 8,
    class_mode="categorical"
)

# 生成测试数据
test_generator = valid_test_datagen.flow_from_dataframe(
    dataframe=test_df,
    x_col="filepath",
    y_col="label",
    target_size=(128,128),
    batch_size = 8,
    class_mode="categorical",
    shuffle=False  # 测试数据不打乱顺序
)

# 输出数据量
print(f"Train generator samples: {train_generator.samples}")
print(f"Valid generator samples: {valid_generator.samples}")
print(f"Test generator samples: {test_generator.samples}")
print(train_df['label'])
# 输出结果
print(f"Train set - defect: {train_with_defect}, no_defect: {train_without_defect}")
print(f"Valid set - defect: {valid_with_defect}, no_defect: {valid_without_defect}")
print(f"Test set - defect: {test_with_defect}, no_defect: {test_without_defect}")



import tensorflow as tf 
print(tf.version)


from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import backend as K
import tensorflow as tf
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        tf.config.experimental.set_virtual_device_configuration(
            gpus[0],
            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]  # 限制为 4GB
        )
        print("GPU 内存限制已设置！")
    except RuntimeError as e:
        print(e)

def inception_module(x, filters):
    # 1x1卷积分支
    conv1x1 = Conv2D(filters, (1,1), padding='same', activation='relu')(x)
    
    # 3x3卷积分支
    conv3x3 = Conv2D(filters, (3,3), padding='same', activation='relu')(x)
    
    # 5x5卷积分支
    conv5x5 = Conv2D(filters, (5,5), padding='same', activation='relu')(x)
    
    # 池化分支
    pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(x)
    pool = Conv2D(filters, (1,1), padding='same', activation='relu')(pool)
    
    # 合并各分支
    merged = concatenate([conv1x1, conv3x3, conv5x5, pool], axis=-1)
    return merged

def attention_block(inputs):
    attention = Dense(1, activation='tanh')(inputs)    # (batch_size, seq_length, 1)
    attention = Flatten()(attention)                   # (batch_size, seq_length)
    attention = Activation('softmax')(attention)       # (batch_size, seq_length)
    attention = Reshape((inputs.shape[1], 1))(attention)  # (batch_size, seq_length, 1)
    
    context = Multiply()([inputs, attention])          # (batch_size, seq_length, features)
    return context


# input
inputs = Input(shape=(128,128, 3))

# Inception
x = inception_module(inputs, 32)
x = MaxPooling2D(2)(x)
x = inception_module(x, 64)
x = MaxPooling2D(2)(x)
x = inception_module(x, 128)
x = GlobalAveragePooling2D()(x)


x = Reshape((-1, 128))(x)  

# BiLSTM
x = Bidirectional(LSTM(64, return_sequences=True))(x)
x = Dropout(0.3)(x)
# attention
x = attention_block(x)
x = GlobalAveragePooling1D()(x)

# 分类层
x = Dense(64, activation='relu')(x)
x = Dropout(0.5)(x)
outputs = Dense(2, activation='softmax')(x) 

# 构建模型
model = Model(inputs=inputs, outputs=outputs)

model.summary()



from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import Precision, Recall

model.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss='categorical_crossentropy',
    metrics=['accuracy', 
             Precision(name='precision'), 
             Recall(name='recall'), 
             'AUC']
)


from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import tensorflow as tf
# import warnings
# warnings.filterwarnings("ignore", category=tf.keras.utils.generic_utils.CustomMaskWarning)


# assume epochs and bacth size
epochs = 10
batch_size = 8 

steps_per_epoch = train_generator.samples // batch_size
validation_steps = valid_generator.samples // batch_size

from tensorflow.keras.callbacks import ReduceLROnPlateau

callbacks = [
    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),
    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min', verbose=1),  # 保存验证集损失最小的模型
    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=1e-6)
]


model_checkpoint = ModelCheckpoint(
    filepath='best_model.h5',  # 保存最优模型的路径
    monitor='val_loss',
    save_best_only=True,
    mode='min',
    verbose=1
)

history = model.fit(
    train_generator,
    steps_per_epoch=steps_per_epoch,  
    validation_data=valid_generator,
    validation_steps=validation_steps, 
    epochs=epochs,
    callbacks=callbacks
)



import matplotlib.pyplot as plt

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(epochs, acc, label='Training Accuracy')
plt.plot(epochs, val_acc, label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs, loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()


def plot_precision_recall(history):
    if 'precision' not in history.history or 'recall' not in history.history:
        print("No precision or recall data found in history!")
        return
    
    plt.figure(figsize=(12, 4))
    
    # Precision plot
    plt.subplot(1, 2, 1)
    plt.plot(history.history['precision_5'], label='Train Precision')
    plt.plot(history.history['val_precision_5'], label='Validation Precision')
    plt.title('Precision')
    plt.xlabel('Epochs')
    plt.ylabel('Precision')
    plt.legend()

    # Recall plot
    plt.subplot(1, 2, 2)
    plt.plot(history.history['recall_5'], label='Train Recall')
    plt.plot(history.history['val_recall_5'], label='Validation Recall')
    plt.title('Recall')
    plt.xlabel('Epochs')
    plt.ylabel('Recall')
    plt.legend()

    plt.show()

plot_precision_recall(history)



    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(history.history['auc'], label='Train auc')
    plt.plot(history.history['val_auc'], label='Validation auc')
    plt.title('AUC')
    plt.xlabel('Epochs')
    plt.ylabel('AUC')
    plt.legend()
    plt.show()


test_loss, test_accuracy, test_precision, test_recall, test_auc = model.evaluate(test_generator, verbose=1)


# Print the metrics
print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")
print(f"Test Precision: {test_precision}")
print(f"Test Recall: {test_recall}")
print(f"Test AUC: {test_auc}")



from sklearn.metrics import confusion_matrix
import numpy as np

test_steps = test_generator.n // test_generator.batch_size
y_true = []
y_pred = []

for _ in range(test_steps):
    x, y = test_generator.next()
    predictions = model.predict(x)
    y_true.extend(np.argmax(y, axis=1))
    y_pred.extend(np.argmax(predictions, axis=1))

cm = confusion_matrix(y_true, y_pred)

# 计算TP, TN, FP, FN
TP = cm[1, 1]
TN = cm[0, 0]
FP = cm[0, 1]
FN = cm[1, 0]

sensitivity = TP / float(TP + FN)
specificity = TN / float(TN + FP)

print(f'Sensitivity: {sensitivity}')
print(f'Specificity: {specificity}')


from sklearn.metrics import f1_score
import numpy as np

test_steps = test_generator.n // test_generator.batch_size
y_true = []
y_pred = []

for _ in range(test_steps):
    x, y = test_generator.next()
    predictions = model.predict(x)
    y_true.extend(np.argmax(y, axis=1))
    y_pred.extend(np.argmax(predictions, axis=1))

f1 = f1_score(y_true, y_pred, average='binary')  

print(f'F1 Score: {f1}')


from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt
import numpy as np

test_steps = test_generator.n // test_generator.batch_size
y_true = np.array([])
y_scores = np.array([])

for _ in range(test_steps):
    x, y = test_generator.next()
    predictions = model.predict(x)
    y_true = np.append(y_true, np.argmax(y, axis=1))
    y_scores = np.append(y_scores, predictions[:, 1])  # 假设第二列为阳性类别的预测概率

precision, recall, thresholds = precision_recall_curve(y_true, y_scores)

plt.figure(figsize=(6, 4))
plt.plot(recall, precision, label='Precision-Recall curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.show()


from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np

test_steps = test_generator.n // test_generator.batch_size
y_true = np.array([])
y_scores = np.array([])

for _ in range(test_steps):
    x, y = test_generator.next()
    predictions = model.predict(x)
    y_true = np.append(y_true, np.argmax(y, axis=1))
    y_scores = np.append(y_scores, predictions[:, 1])  # 假设第二列为阳性类别的预测概率

fpr, tpr, thresholds = roc_curve(y_true, y_scores)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc="lower right")
plt.show()


#XAI-GRAD CAM
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing import image
import tensorflow as tf

def gamma_correction(image, gamma=1.0):
    return np.power(image, gamma)

def linear_stretch(image):
    min_val = np.min(image)
    max_val = np.max(image)
    stretched = (image - min_val) / (max_val - min_val) * 255
    return stretched.astype(np.uint8)

def get_img_array(img_path, size):
    img = image.load_img(img_path, target_size=size)
    array = image.img_to_array(img)
    array = np.expand_dims(array, axis=0)
    return array * 1./255  # Match the rescale factor used in ImageDataGenerator

def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    grad_model = tf.keras.models.Model(
        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]
    )

    with tf.GradientTape() as tape:
        last_conv_layer_output, preds = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(preds[0])
        class_channel = preds[:, pred_index]

    grads = tape.gradient(class_channel, last_conv_layer_output)

    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    last_conv_layer_output = last_conv_layer_output[0]
    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)

    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()


def save_and_display_gradcam(img_path, heatmap, cam_path="cam.jpg", alpha=0.4, gamma=1.0):
    img = image.load_img(img_path)
    img = image.img_to_array(img)

    heatmap = np.uint8(255 * heatmap)

    heatmap = gamma_correction(heatmap, gamma)

    heatmap = linear_stretch(heatmap)

    jet = plt.get_cmap("jet")
    jet_colors = jet(np.arange(256))[:, :3]
    jet_heatmap = jet_colors[heatmap]

    jet_heatmap = image.array_to_img(jet_heatmap)
    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))
    jet_heatmap = image.img_to_array(jet_heatmap)

    superimposed_img = jet_heatmap * alpha + img
    superimposed_img = image.array_to_img(superimposed_img)

    superimposed_img.save(cam_path)


    fig, ax = plt.subplots(1, 2, figsize=(12, 6))

    # 原始图像
    ax[0].imshow(img / 255.0)  # Normalize to [0, 1] range for displaying
    ax[0].set_title("Original Image")
    ax[0].axis('off')

    # 叠加热图的图像
    ax[1].imshow(superimposed_img)
    ax[1].set_title("Grad-CAM Heatmap")
    ax[1].axis('off')

    plt.tight_layout()
    plt.show()

    return cam_path

img_path = r"C:\Users\xiaog\Desktop\Project\Datasets\dataset\dataset 4\Data Files\train\3_jpg.rf.30701df206b2f1ae280bf1d1b050e85d.jpg"
img_array = get_img_array(img_path, size=(128,128))  # 使用正确的输入尺寸
heatmap = make_gradcam_heatmap(img_array, model, 'conv2d_107', pred_index=0)  # 使用正确的层名称
cam_path = save_and_display_gradcam(img_path, heatmap, gamma=1.5)  # 使用伽马校正








for layer in model.layers[::-1]:  # 逆序遍历
    if 'conv' in layer.name:
        print("The last layer:", layer.name)
        break


















































































